{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "from transformers import TFBertModel, BertTokenizer\n",
    "from transformers import Trainer\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datasets\n",
    "import pyarrow as pa\n",
    "import pyarrow.dataset as ds\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создание датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "import psycopg2\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "config = dotenv_values(\".env\")\n",
    "LOGIN = config[\"login\"]\n",
    "PASSWORD = config[\"password\"]\n",
    "DATABASE = config[\"database\"]\n",
    "\n",
    "tables = ['rf_subjects', 'rf_regions', 'rf_cities', 'addresses', 'jobs', 'departments', 'directions', 'sections', 'specialites', 'highscools', 'diplomas', 'mil_ranks', 'mil_specs', 'militaries', 'employee', 'paytypes', 'pays']\n",
    "\n",
    "with psycopg2.connect(database=DATABASE, user=LOGIN, password=PASSWORD) as connection:\n",
    "    with connection.cursor() as cur:\n",
    "        for table in tables:\n",
    "            cur.execute(f'SELECT * FROM {table}')\n",
    "            data = cur.fetchall()\n",
    "\n",
    "            with open(f'{table}.csv', 'w') as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([desc[0] for desc in cur.description])\n",
    "                for row in data:\n",
    "                    writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data)\n",
    "\n",
    "data = datasets.Dataset(pa.Table.from_pandas(df))\n",
    "\n",
    "train_test_dataset = data.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание модели\n",
    "Модель будет входящий текст, и определять принадлежность каждого токена группе чувствительных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerClassificationModel(nn.Module):\n",
    "    def __init__(self, model_name, base_transformer_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.backbone = base_transformer_model\n",
    "        self.name = model_name\n",
    "        self.in_dim = self.backbone.pooler.dense.out_features if self.name == \"model1\" else self.backbone.bert.output_shape\n",
    "        self.linlayers = nn.Sequential(\n",
    "            nn.Linear(self.in_dim, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(32, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs, x):\n",
    "        out = self.backbone(x)\n",
    "        outputs = self.linlayers(out)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_backbone_function(model: TransformerClassificationModel):\n",
    "    if model.name == \"model1\":\n",
    "        for params in model.backbone.parameters():\n",
    "            params.requires_grad = False\n",
    "    else:\n",
    "        for params in model.backbone.bert.trainable_variables:\n",
    "            params.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(examples, max_length=1024, truncation=True)\n",
    "\n",
    "    labels = tokenizer(text_target=examples, max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Тренировка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_transformer(transformer_model, train_dataset, eval_dataset, freeze_backbone=True, tokenizer=None, data_collator=None):\n",
    "    model = copy.copy(transformer_model)\n",
    "    if freeze_backbone == True:\n",
    "        freeze_backbone_function(model)\n",
    "    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n",
    "    tokenized_eval = eval_dataset.map(preprocess_function, batched=True)\n",
    "    data_collator = transformers.DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "\n",
    "    training_args = transformers.TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=4,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=2,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train,\n",
    "        eval_dataset=tokenized_eval,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    return trainer.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['name', 'capital', 'region', 'city', 'street', 'house', 'flat', 'boss', 'dept', 'seria', 'passport', 'birthdate date', 'age', 'surname', 'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"tbs17/MathBERT\")\n",
    "backbone = TFBertModel.from_pretrained(\"tbs17/MathBERT\", from_pt=True)\n",
    "math_bert = TransformerClassificationModel(\"model2\", backbone, classes)\n",
    "math_bert_freeze_finetuned = train_transformer(transformer_model=math_bert, train_dataset=train_test_dataset['train'], eval_dataset=train_test_dataset['test'], freeze_backbone=True, tokenizer=tokenizer)\n",
    "\n",
    "rubert_tiny_transformer_model = TransformerClassificationModel(\"model2\", backbone, classes)\n",
    "math_bert_full_finetuned = train_transformer(math_bert, train_dataset=train_test_dataset['train'], eval_dataset=train_test_dataset['test'], freeze_backbone=False, tokenizer=tokenizer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
